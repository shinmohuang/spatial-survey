"""
ç§‘ç ”çº§çŸ©é˜µå–æ ·æµ‹å·ç”Ÿæˆè„šæœ¬
å®ç° PISA/TIMSS é£æ ¼çš„æµ‹å·è®¾è®¡ï¼šç±»åˆ«å‡è¡¡ + é‡å é¢˜ + IRTé“¾æ¥
"""
import pandas as pd
import numpy as np
import json
import pathlib
from collections import defaultdict
import base64

# ===== æ ¸å¿ƒå‚æ•° =====
CSV = "/Volumes/Disk1T/DISE-dataset/DISE-bench/DISE-benchmark.csv"      # åŒ…å«é¢˜ç›®å†…å®¹å’Œå›¾ç‰‡è·¯å¾„çš„æ–‡ä»¶
IMAGE_BASE_PATH = pathlib.Path("/Volumes/Disk1T/DISE-dataset/")  # å›¾ç‰‡æ–‡ä»¶çš„æ ¹ç›®å½•
I = 559                    # é¢˜ç›®æ€»æ•°
C = 10                     # ç±»åˆ«æ•°
K = 30                     # æ¯äººåšçš„é¢˜ç›®æ•°ï¼ˆæ¯ä»½æµ‹å·é¢˜æ•°ï¼‰
OVERLAP_ITEMS = 3          # é‡å é¢˜æ•°é‡ï¼ˆç”¨äºIRTé“¾æ¥ï¼‰
SEED = 42


def encode_image_to_base64(image_path):
    """è¯»å–å›¾ç‰‡æ–‡ä»¶å¹¶ç¼–ç ä¸ºBase64 Data URI"""
    if pd.isna(image_path) or not image_path.strip():
        return None
    try:
        # è¿™é‡Œçš„ image_path æ˜¯ "DISE-dataset/images/..."ï¼Œæ‰€ä»¥æˆ‘ä»¬ç›´æ¥ç”¨æ ¹è·¯å¾„
        full_image_path = IMAGE_BASE_PATH / \
            image_path.replace("DISE-dataset/", "", 1)
        ext = full_image_path.suffix.lstrip('.').lower()
        if ext == 'jpg':
            ext = 'jpeg'

        with open(full_image_path, "rb") as image_file:
            encoded_string = base64.b64encode(
                image_file.read()).decode('utf-8')
            return f"data:image/{ext};base64,{encoded_string}"
    except (FileNotFoundError, IOError) as e:
        print(f"âš ï¸  è­¦å‘Š: æ— æ³•è¯»å–å›¾ç‰‡æ–‡ä»¶ {image_path}ã€‚é”™è¯¯: {e}")
        return None


def create_scientific_booklets():
    """ç”Ÿæˆç§‘å­¦çš„çŸ©é˜µå–æ ·æµ‹å·"""

    # 1. è¯»å–åŒ…å«å›¾ç‰‡è·¯å¾„çš„æ•°æ®
    print("ğŸ“š è¯»å– DISE æ•°æ®é›†ï¼ˆåŒ…å«å›¾ç‰‡è·¯å¾„ï¼‰...")
    df = pd.read_csv(CSV)

    # 2. å°†å›¾ç‰‡è·¯å¾„å®æ—¶ç¼–ç ä¸ºbase64
    print("ğŸ–¼ï¸  æ­£åœ¨å°†å›¾ç‰‡ç¼–ç ä¸º base64...")
    df['image'] = df['image'].apply(encode_image_to_base64)

    # ç¡®ä¿æœ‰å›¾ç‰‡æ•°æ®åˆ—
    if 'image' not in df.columns:
        print("âŒ é”™è¯¯ï¼šæ•°æ®æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°imageåˆ—")
        return

    print(f"æ€»é¢˜æ•°: {len(df)}, ç±»åˆ«æ•°: {df['category'].nunique()}")

    # æ£€æŸ¥å›¾ç‰‡æ•°æ®æ ¼å¼
    sample_image = df['image'].iloc[0] if len(df) > 0 else None
    if sample_image and not str(sample_image).startswith('data:'):
        print("ğŸ“ å›¾ç‰‡æ•°æ®éœ€è¦è½¬æ¢ä¸ºdata URIæ ¼å¼...")
    else:
        print("âœ… å›¾ç‰‡æ•°æ®å·²ç»æ˜¯æ­£ç¡®çš„æ ¼å¼")

    # 3. æŒ‰ç±»åˆ«ã€éš¾åº¦åˆ†å±‚éšæœºæ’åºï¼ˆä¿è¯æ¯ç±»åˆ«åœ¨å„æµ‹å·ä¸­å‡åŒ€åˆ†å¸ƒï¼‰
    print("ğŸ”€ æŒ‰ç±»åˆ«å’Œéš¾åº¦åˆ†å±‚æ’åº...")
    df = df.sort_values(['category', 'difficulty'], na_position='last')
    df = df.groupby('category').apply(lambda x: x.sample(
        frac=1, random_state=SEED)).reset_index(drop=True)

    # 4. è®¡ç®—æµ‹å·æ•°é‡
    B = int(np.ceil(len(df) / K))
    print(f"ğŸ“– ç”Ÿæˆ {B} ä¸ªæµ‹å·ï¼Œæ¯å· {K} é¢˜")

    # 5. åˆ›å»ºå‡è¡¡çš„æµ‹å·åˆ†é…
    booklets = create_balanced_booklets(df, B, K)

    # 6. æ·»åŠ é‡å é¢˜ï¼ˆç”¨äºIRTé“¾æ¥ï¼‰
    booklets = add_linking_items(booklets, OVERLAP_ITEMS)

    # 7. è¾“å‡ºæµ‹å·æ–‡ä»¶
    save_booklets(booklets)

    # 8. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    generate_stats_report(booklets, df)


def create_balanced_booklets(df, B, K):
    """åˆ›å»ºç±»åˆ«å‡è¡¡çš„æµ‹å·"""
    booklets = {i: [] for i in range(B)}

    # æŒ‰ç±»åˆ«å¾ªç¯åˆ†é…ï¼Œç¡®ä¿æ¯ä¸ªæµ‹å·çš„ç±»åˆ«åˆ†å¸ƒå‡åŒ€
    categories = df['category'].unique()
    items_per_category = K // len(categories)  # æ¯ç±»åˆ«åœ¨å•æµ‹å·ä¸­çš„é¢˜æ•°

    print(f"ğŸ“Š æ¯ä¸ªæµ‹å·åŒ…å« {len(categories)} ä¸ªç±»åˆ«ï¼Œæ¯ç±»çº¦ {items_per_category} é¢˜")

    for cat in categories:
        cat_items = df[df['category'] == cat].copy()

        # å¾ªç¯åˆ†é…è¯¥ç±»åˆ«çš„é¢˜ç›®åˆ°å„æµ‹å·
        for i, (_, item) in enumerate(cat_items.iterrows()):
            booklet_id = i % B
            if len(booklets[booklet_id]) < K:  # é˜²æ­¢è¶…å‡ºæ¯å·é¢˜æ•°é™åˆ¶
                item_dict = item.to_dict()
                item_dict['booklet_id'] = booklet_id
                item_dict['position'] = len(booklets[booklet_id]) + 1

                # å°†base64å›¾ç‰‡æ•°æ®æ ¼å¼åŒ–ä¸ºdata URI
                if pd.notna(item_dict.get('image')):
                    image_data = item_dict['image']
                    # å¦‚æœä¸æ˜¯data URIæ ¼å¼ï¼Œè½¬æ¢ä¸ºdata URI
                    if not str(image_data).startswith('data:'):
                        item_dict['image'] = f"data:image/png;base64,{image_data}"
                    # å¦‚æœå·²ç»æ˜¯data URIæ ¼å¼ï¼Œä¿æŒä¸å˜
                else:
                    item_dict['image'] = None

                booklets[booklet_id].append(item_dict)

    return booklets


def add_linking_items(booklets, overlap_count):
    """æ·»åŠ é‡å é¢˜å®ç°æµ‹å·é—´IRTé“¾æ¥"""
    print(f"ğŸ”— æ·»åŠ  {overlap_count} ä¸ªé‡å é¢˜è¿›è¡ŒIRTé“¾æ¥...")

    B = len(booklets)

    # ä¸ºæ¯ä¸ªæµ‹å·æ·»åŠ æ¥è‡ªå‰ä¸€ä¸ªæµ‹å·çš„é‡å é¢˜
    for i in range(1, B):  # ä»ç¬¬2ä¸ªæµ‹å·å¼€å§‹
        prev_booklet = booklets[i-1]
        current_booklet = booklets[i]

        # å–å‰ä¸€æµ‹å·çš„æœ€åå‡ é¢˜ä½œä¸ºå½“å‰æµ‹å·çš„é“¾æ¥é¢˜
        linking_items = prev_booklet[-overlap_count:].copy() if len(
            prev_booklet) >= overlap_count else prev_booklet.copy()

        for item in linking_items:
            item_copy = item.copy()
            item_copy['is_linking'] = True
            item_copy['original_booklet'] = i-1
            item_copy['position'] = len(current_booklet) + 1
            current_booklet.append(item_copy)

    return booklets


def save_booklets(booklets):
    """ä¿å­˜æµ‹å·åˆ°æ–‡ä»¶"""
    out = pathlib.Path("public/booklets")
    out.mkdir(parents=True, exist_ok=True)

    print(f"ğŸ’¾ ä¿å­˜æµ‹å·åˆ° {out}/...")

    for bid, items in booklets.items():
        # éšæœºæ‰“ä¹±é¢˜ç›®é¡ºåºï¼ˆé˜²æ­¢ä½ç½®æ•ˆåº”ï¼‰
        items_shuffled = items.copy()
        np.random.seed(SEED + bid)  # æ¯ä¸ªæµ‹å·ä½¿ç”¨ä¸åŒçš„éšæœºç§å­
        np.random.shuffle(items_shuffled)

        # é‡æ–°ç¼–å·
        for pos, item in enumerate(items_shuffled, 1):
            item['position'] = pos

        (out / f"{bid}.json").write_text(
            json.dumps(items_shuffled, ensure_ascii=False, indent=2),
            encoding="utf-8"
        )

    print(f"âœ… {len(booklets)} ä¸ªæµ‹å·å·²ç”Ÿæˆ")


def generate_stats_report(booklets, df):
    """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
    print("\nğŸ“ˆ === æµ‹å·ç»Ÿè®¡æŠ¥å‘Š ===")

    # åŸºç¡€ç»Ÿè®¡
    total_items = sum(len(items) for items in booklets.values())
    avg_items_per_booklet = total_items / len(booklets)

    print(f"æµ‹å·æ€»æ•°: {len(booklets)}")
    print(f"æ€»é¢˜æ•°: {total_items}")
    print(f"å¹³å‡æ¯å·é¢˜æ•°: {avg_items_per_booklet:.1f}")

    # ç±»åˆ«åˆ†å¸ƒç»Ÿè®¡
    category_dist = defaultdict(lambda: defaultdict(int))
    for bid, items in booklets.items():
        for item in items:
            category_dist[bid][item.get('category', 'Unknown')] += 1

    print(f"\nğŸ“Š ç±»åˆ«åˆ†å¸ƒï¼ˆæ¯ä¸ªæµ‹å·ï¼‰:")
    categories = df['category'].unique()
    for cat in categories:
        counts = [category_dist[bid][cat] for bid in range(len(booklets))]
        print(
            f"  {cat}: å¹³å‡ {np.mean(counts):.1f} é¢˜ (èŒƒå›´: {min(counts)}-{max(counts)})")

    # é‡å é¢˜ç»Ÿè®¡
    linking_items = sum(1 for items in booklets.values()
                        for item in items if item.get('is_linking', False))
    print(f"\nğŸ”— é“¾æ¥é¢˜æ€»æ•°: {linking_items}")

    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    report = {
        'total_booklets': len(booklets),
        'total_items': total_items,
        'avg_items_per_booklet': avg_items_per_booklet,
        'category_distribution': dict(category_dist),
        'linking_items_count': linking_items,
        'generation_params': {
            'K': K,
            'OVERLAP_ITEMS': OVERLAP_ITEMS,
            'SEED': SEED
        }
    }

    pathlib.Path("public/booklets/stats.json").write_text(
        json.dumps(report, ensure_ascii=False, indent=2),
        encoding="utf-8"
    )

    print("ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ° public/booklets/stats.json")


if __name__ == "__main__":
    np.random.seed(SEED)
    create_scientific_booklets()
    print("\nğŸ‰ ç§‘ç ”çº§æµ‹å·ç”Ÿæˆå®Œæˆï¼")
